@cindex Monte Carlo integrators
@menu
* Algorithms::                  
* Interface::                   
* Example::                     
* The Future::                  
@end menu

@c
@c RCS: $Id$
@c

This chapter describes the Monte Carlo integration routines in the
library.  At the moment the algorithms implemented are: plain non-adaptive,
 Monte Carlo, VEGAS (following Peter Lepage) and MISER (following
Numerical recipes).  We will dispense with an introduction and simply
describe the algorithms and interfaces.  Note that the documentation (as
well as the code) is still evolving - the code at a faster rate.

@node Algorithms
@section Algorithms

@subsection PLAIN (or Simple) Monte Carlo

We begin by establishing some notation.  Let @math{E_\mu(f)} denote the
expactation value (or average) of the function @math{f} with respect
to some measure @math{\mu}.  We will write @math{E_L(f)} for the
normal (or Lebesgue) integral and simple @math{E(f)} for a Monte Carlo
estimate.  Specifically then,
@equation
  E(f) = @{1 \over N @} \sum_i^N f(x_i).
@end equation
Similarly, we will denote the variance of the expactation value
as @math{Var_@{\mu@}(f)}, defined as 
@equation
  Var_\mu(f) = E_\mu(f^2) - (E_\mu(f))^2.
@end equation
The fundamental point of Monte Carlo integration is that for
large @math{N}, 
@equation
  E_L(f) - E(f) \approx @{Var_L(f)\over N @}
@end equation
and furthermore, 
@equation
  Var(f) \approx @{ Var(f) \over N@}
@end equation
That's all there is to simple Monte Carlo.


@subsection MISER

The starting point of all stratified sampling techniques is
the observation that for two disjoint regions @math{a} and @math{b} with
Monte Carlo estimates of the integral @math{E_a(f)} and @math{E_b(f)} of the 
integral of f in those regions
and variances @math{Var_a(f)} and @math{Var_b(f)}, the variance @math{Var(f)}
of the combined estimate @math{E(f) = 1/2 E_a(f) + 1/2 E_b(f)}
is given by
@equation
  Var(f) = @{Var_@{L,a@}(f) \over 4 N_a@} + @{Var_@{L,b@}(f) \over 4 N_b@}
@end equation
This variance 
is minimized (subject to the constrain that @math{N_a + N_b} is fixed) 
by choosing (assuming one can) @math{N_a} s.t.
@equation
  @{N_a \over N_a+N_b @} = @{ \sigma_a \over \sigma_a + \sigma_b @}
@end equation
(where we have written @math{\sigma} for @math{Var_L(f)}).  
For such a choice, the variance is given by
@equation
  Var(f) =  @{ (\sigma_a + \sigma_b)^2 \over 4 N @}
@end equation
In words, 
the variance of the estimate (and hence the error) is minimized by
concentrating points in regions where the variance of the function
@math{f} is large.

The most straightforward stratified sampling routine divides a
hypercubic integration region into sub-cubss along the coordinate axes.
This is simple to do, but gets out of hand for large numbers of
dimensions @math{d} because the number of sub-cubes grows like 
@math{K^d} where @math{K} is the number of sub-divisions along the
axis.  

What MISER does (and other recursive strafied samplers) is to bisect
the hypercube along one coordinate axis.  The direction is chosen
by examining all possible bisections (@math{d} of them) and picking
the one that gives the best combined variance.  The same procedure
is then done for each of the two half-spaces (chosing @math{N_a} and
@math{N_b} as described above), and so on. 
Since by assumption one does not know the variance @math{Var_L(f)} in
the various sub-regions, it is estimated using some fraction of
the total number of points alloted to the estimate.  

@subsection VEGAS


@node Interface
@section Interface

All of the integration routines use the same interface.  There
is an allocator to allocate memory to hold control variables and
workspace, a routine to initialize those control variables,
the integrator itself, and of course a function to free the space
when done.  For an integrator algorithm, call it COOL
(substitute any of the currently existing algorithms) we then 
have
@deftypefun gsl_monte_COOL_state* gsl_monte_COOL_alloc()
@end deftypefun
@deftypefun int gsl_monte_COOL_init(gsl_monte_COOL_state* @var{s})
@end deftypefun
@deftypefun int gsl_monte_COOL_free(gsl_monte_COOL_state* @var{s}), 
@end deftypefun
@deftypefun int gsl_monte_COOL_validate(gsl_monte_COOL_state* @var{s}, 
        double* @var{x_lower}, double* @var{x_upper}, 
        unsigned long @var{dimension}, unsigned long @var{function_calls})
@end deftypefun
@deftypefun int gsl_monte_COOL_integrate(gsl_monte_COOL_state* @var{s}, 
        double* @var{x_lower}, double* @var{x_upper}, 
        unsigned long @var{dimension}, unsigned long @var{function_calls}, 
        double* @var{result}, double* @var{error},
        ...)
@end deftypefun

Notice the ellipses in last argument to the actual integration routine.
This is because the vegas algorithm (and perhaps others in the future)
return extra information -- in the case of vegas, it is the "chisq"
of the result.  

In addition to the common function interface, the routines also share
the state variable 
@deftypevr {Control} gsl_rng* ranf
which determines which random number generator will be used.
@end deftypevr

We describe the algorithm-specific variables.  

PLAIN: None

MISER:
@deftypevr {Control} double alpha
alpha controls how the variances for the two sub-regions are combined.
The Numberical Recipes gang argue that for recursive sampling there
is no reason to expect that the variance should scale like @math{1/N} and
so they allow the scaling to depend on @math{\alpha}
@equation 
  Var(f) = @{\sigma_a \over N_a^\alpha @} + 
               @{\sigma_b \over N_b^\alpha @}
@end equation
@end deftypevr

@deftypevr {Control} double dither
Rather than exactly bisection the integration region, dither allows the
user to introduce a bit of fuzz.  This helps in the case when the
function to be integrated has some symmetry, say if it is peaked in the
center of the hypercube.  If needed, dither should be around 0.1.
@end deftypevr 

VEGAS:
@deftypevr {Control} double alpha
For Vegas, @code{alpha} controls the stiffness of the rebinning algorithm:
@code{alpha = 0} means never rebin.  It is typically set between 
one and two.
@end deftypevr
@deftypevr {Control} double acc
Setting @code{acc} allows vegas to terminate when the desired accuracy
has been achieved, rather than after a certain number of function calls.
@end deftypevr
@deftypevr {Control} {long int} max_it_num
The maximum number of iterations to perform.
@end deftypevr
@deftypevr {Control} int verbose
Whether to print information about the calculation.
@end deftypevr
@deftypevr {Control} int stage
Setting this determines the "stage" of the calculation.  Normally,
@code{stage = 0}.  Calling vegas @code{stage = 1} skips ... FIXME
@end deftypevr
@deftypevr {Control} int mode
The possible choices are @code{GSL_VEGAS_MODE_IMPORTANCE}, 
        @code{GSL_VEGAS_MODE_STRATIFIED}, 
        @code{GSL_VEGAS_MODE_IMPORTANCE_ONLY}. 
This determines whether vegas will use importance sampling or stratified
sampling, or whether it can pick on its own.  The p
@end deftypevr



@node Example
@section Example

Here we provide s simple example of using the integration routines.  Vegas
was chosen at random.

@example
#include <math.h>
#include <stdio.h>

#include <gsl_math.h>
#include <gsl_monte_vegas.h>

double f1(double x[]) 
@{
  int i;
  double product = 1.0;

  for (i = 0; i < 10; i++) 
    product *= x[i];

  return product;
@}

main () 
@{
  double xl[10] = @{0, 0, 0, 0, 0, 0, 0, 0, 0, 0@};
  double xu[10] = @{1, 1, 1, 1, 1, 1, 1, 1, 1, 1@};

  double res = 0;
  double err = 0;
  double chisq = 0;
  int status = 0;
  unsigned long calls = 10000;
  unsigned long dimension = 10;

  gsl_monte_vegas_state* s = gsl_monte_vegas_alloc();
  gsl_monte_plain_init(s);

  status = gsl_monte_vegas_integrat(s, f0, xl, xu, dimension, calls,
                                    &res, &err, &chisq);
  if (status)
    printf("oops!\n");
  else
    printf("vegas(f0) = %f +- %f with \"chisq\" %f\n", res, err, chisq);

@}
@end example


@node The Future
@section The Future

In the future, the author of the Monte Carlo routines intends to add
more algorithms, greater control over the current ones,  more error
handling, and a more consistent interface.  Probably something like
the rng interface will evolve, so that something like 
@example
  integrator = gsl_monte_alloc(gsl_monte_vegas, dimension)
@end example 
will be possible.
The old interface will probably stay around for a while (though, since
this is @math{version < 1}, the author 
does not want to be held to such as statement).

@c
@c @display
@c   To Be Fixed.
@c @end display
@c @noindent
@c 
@c @itemize @asis
@c @item 
@c @code{item 1}
@c @item 
@c @code{item 2}
@c @end itemize
@c 
