@cindex Monte Carlo integration

This chapter describes routines for multidimensional Monte Carlo
integration.  These include the traditional Monte Carlo method of pure
random sampling and adaptive algorithms such as @sc{vegas} and
@sc{miser}, which use importance sampling and stratified sampling.

Each algorithm computes an estimate of a definite integral of the form,

@tex
\beforedisplay
$$
I = \int_{xl}^{xu} \int_{yl}^{yu} ... dx\,dy ... f(x,y,...)
$$
\afterdisplay
@end tex
@ifinfo
@example
I = \int_xl^xu \int_yl^yu ... dx dy ..  f(x, y, ...)
@end example
@end ifinfo
@noindent
over a hypercubic region (xl,xu) (yl,yu) ... using a fixed number of
function calls specified by the user. Each routine also provides an
estimate of the error on the result.  These error estimates should be
taken as a guide rather than as a strict error bound, since random
sampling of the region may not uncover all the important features of the
function.

@menu
* Monte Carlo Interface::       
* PLAIN (or Simple) Monte Carlo::  
* MISER::                       
* VEGAS::                       
* Monte Carlo Examples::        
* Monte Carlo Integration References and Further Reading::  
@end menu


@node Monte Carlo Interface
@section Interface
All of the Monte Carlo integration routines use the same interface.
There is an allocator to allocate memory to hold control variables and
workspace, a routine to initialize those control variables, the
integrator itself, and a function to free the space when done.  Each
integration function requires a random number generator to be supplied.




Notice the ellipses in the last argument to the actual integration routine.
This is because the vegas algorithm (and perhaps others in the future)
returns extra information -- in the case of vegas, it is the @math{\chi^2}
of the result.  

In addition to the common function interface, the routines also share
the state variables 
@deftypevr {Control} gsl_rng* ranf
which determines which random number generator will be used.
@end deftypevr
and
@deftypevr {Control} int verbose
which says whether to print information about the calculation (though
the actual use depends on the algorithm).
@end deftypevr

@node PLAIN (or Simple) Monte Carlo
@section PLAIN (or Simple) Monte Carlo
@cindex plain monte carlo
We begin by establishing some notation.  Let @math{I(f)} denote the
integral of the function @math{f} (for convenience, we take @math{I(1)=1})
We will write the Monte Carlo estimate of the integral as
@math{E(f; N)} for the

@tex
\beforedisplay
$$
E(f; N) = {1 \over N} \sum_i^N f(x_i).
$$
\afterdisplay
@end tex
@ifinfo
@example
E(f; N) = @{1 \over N @} \sum_i^N f(x_i).
@end example
@end ifinfo
@noindent
for @math{N} randomly distributed points @math{x_i} Similarly, we will
denote the variance of @math{f} by

@tex
\beforedisplay
$$
\sigma^2(f) = I(f^2) - I(f)^2
$$
\afterdisplay
@end tex
@ifinfo
@example
\sigma^2(f) = I(f^2) - I(f)^2
@end example
@end ifinfo
@noindent
and the variance of the estimate by 

@tex
\beforedisplay
$$
Var(f; N) = E(f^2; N) - (E(f; N))^2.
$$
\afterdisplay
@end tex
@ifinfo
@example
Var(f; N) = E(f^2; N) - (E(f; N))^2.
@end example
@end ifinfo

The fundamental point of Monte Carlo integration is that for
large @math{N}, 

@tex
\beforedisplay
$$
E(f; N) - I(f) \approx {\sigma^2(f)\over N}
$$
\afterdisplay
@end tex
@ifinfo
@example
E(f; N) - I(f) \approx @{\sigma^2(f)\over N @}
@end example
@end ifinfo
@noindent
and furthermore, by the same argument, 

@tex
\beforedisplay
$$
\sigma^2(f) \approx Var(f; N)
$$
\afterdisplay
@end tex
@ifinfo
@example
\sigma^2(f) \approx Var(f; N)
@end example
@end ifinfo

That's all there is to simple Monte Carlo.

@deftypefun {gsl_monte_plain_state *} gsl_monte_plain_alloc (size_t @var{dim})

@end deftypefun

@deftypefun int gsl_monte_plain_init (gsl_monte_plain_state* @var{s})
@end deftypefun

@deftypefun int gsl_monte_plain_integrate (gsl_monte_function * @var{f}, double * @var{xl}, double * @var{xu}, size_t @var{dim}, size_t @var{calls}, gsl_rng * @var{r}, gsl_monte_plain_state * @var{s}, double * @var{result}, double * @var{abserr})
@end deftypefun

@deftypefun void gsl_monte_plain_free (gsl_monte_plain_state* @var{s}), 
@end deftypefun

@node MISER
@section MISER
@cindex MISER monte carlo integration
@cindex recursive stratified sampling, MISER

The starting point of all stratified sampling techniques is
the observation that for two disjoint regions @math{a} and @math{b} with
Monte Carlo estimates of the integral @math{E_a(f)} and @math{E_b(f)} of the 
integral of f in those regions
and variances @math{\sigma_a^2(f)} and @math{\sigma_b^2(f)}, 
the variance @math{Var(f)}
of the combined estimate 
@c{$E(f) = {1\over 2} E_a(f) + {1\over 2} E_b(f)$}
@math{E(f) = @{1\over 2@} E_a(f) + @{1\over 2@} E_b(f)}
is given by

@tex
\beforedisplay
$$
Var(f) = {\sigma_a^2(f) \over 4 N_a} + {\sigma_b^2(f) \over 4 N_b}
$$
\afterdisplay
@end tex
@ifinfo
@example
Var(f) = @{\sigma_a^2(f) \over 4 N_a@} + @{\sigma_b^2(f) \over 4 N_b@}
@end example
@end ifinfo

This variance is minimized (subject to the constrain that @math{N_a + N_b} 
is fixed) by choosing (assuming one can) @math{N_a} s.t.

@tex
\beforedisplay
$$
{N_a \over N_a+N_b} = {\sigma_a \over \sigma_a + \sigma_b}
$$
\afterdisplay
@end tex
@ifinfo
@example
@{N_a \over N_a+N_b @} = @{ \sigma_a \over \sigma_a + \sigma_b @}
@end example
@end ifinfo

For such a choice, the variance is given by

@tex
\beforedisplay
$$
Var(f) = {(\sigma_a + \sigma_b)^2 \over 4 N}
$$
\afterdisplay
@end tex
@ifinfo
@example
Var(f) =  @{ (\sigma_a + \sigma_b)^2 \over 4 N @}
@end example
@end ifinfo

In words, the variance of the estimate (and hence the error) is
minimized by concentrating points in regions where the variance of the
function @math{f} is large.

The most straightforward stratified sampling routine divides a
hyper-cubic integration region into sub-cubes along the coordinate axes.
This is simple to do, but gets out of hand for large numbers of
dimensions @math{d} because the number of sub-cubes grows like 
@math{K^d} (where @math{K} is the number of sub-divisions along the
axis).  

What MISER does (and other recursive stratified samplers) is to bisect
the hypercube along one coordinate axis.  The direction is chosen by
examining all possible bisections (@math{d} of them) and picking the one
that gives the best combined variance.  The same procedure is then done
for each of the two half-spaces (choosing @math{N_a} and @math{N_b} as
described above), and so on.  Since by assumption one does not know the
variance @math{Var_L(f)} in the various sub-regions, it is estimated
using some fraction of the total number of points alloted to the
estimate.

MISER:
@deftypevr {Control} double alpha
alpha controls how the variances for the two sub-regions are combined.
The Numerical Recipes gang argue that for recursive sampling there
is no reason to expect that the variance should scale like @math{1/N} and
so they allow the scaling to depend on @math{\alpha}

@tex
\beforedisplay
$$
Var(f) = {\sigma_a \over N_a^\alpha} + {\sigma_b \over N_b^\alpha}
$$
\afterdisplay
@end tex
@ifinfo
@example
Var(f) = @{\sigma_a \over N_a^\alpha@} + @{\sigma_b \over N_b^\alpha@}
@end example
@end ifinfo

@end deftypevr

@deftypevr {Control} double dither
Rather than exactly bisection the integration region, dither allows the
user to introduce a bit of fuzz.  This helps in the case when the
function to be integrated has some symmetry, say if it is peaked in the
center of the hypercube.  If needed, dither should be around 0.1.
@end deftypevr 


@node VEGAS
@section VEGAS
@cindex VEGAS monte carlo integration
@cindex importance sampling, VEGAS

Vegas takes another approach to obtaining good results, namely it
tries to sample points from the probability distribution described
by the function @math{f}.  More precisely, suppose we estimate
the integral of @math{f} with points distributed according to
a probability distribution described by the function @math{g}.  If
we call the new estimate @math{E_g(f; N)} we have

@tex
\beforedisplay
$$
E_g(f; N) = E(f/g; N)
$$
\afterdisplay
@end tex
@ifinfo
@example
E_g(f; N) = E(f/g; N)
@end example
@end ifinfo

and similarly

@tex
\beforedisplay
$$
Var_g(f; N) = Var(f/g; N)
$$
\afterdisplay
@end tex
@ifinfo
@example
Var_g(f; N) = Var(f/g; N)
@end example
@end ifinfo

It is clear from this that if @math{g = |f|/I(|f|)} then the variance 
@math{V_g(f; N)} vanishes.
This it was what Vegas attempts to do.  It makes several passes,
histograming @math{f}, and each time using the histogram to define the
sampling distribution for the next pass.  This basic idea again has the
problem that the number of histogram bins grows like 
@math{K^d}.  The
compromise that Vegas (ie, Peter Lepage) adopts is to assume that
@math{g} factors: 
@c{$g(x_1, x_2, \ldots) = g_1(x_1) g_2(x_2) \ldots$}
@math{g(x_1, x_2, ...) = g_1(x_1) g_2(x_2) ...}
so that the number of bins required is only 
@c{$d \cdot K$}
@math{d.K}.  In this
case, it is not hard to show that the optimal distribution is 

@tex
\beforedisplay
$$ 
g_1(x_1) = [\int dx_1 \ldots dx_n {f^2(x_1,\dots,x_n)\over g_2(x_2)\ldots g_n(x_n) } ]^{(1/2)}
$$
\afterdisplay
@end tex
@ifinfo
@example
g_1(x_1) = [\int dx_1 ... dx_n @{f^2(x_1,...,x_n)\over g_2(x_2)... g_n(x_n) @} ]^@{(1/2)@}
@end example
@end ifinfo

Lepage's Vegas is actually a bit fancier than this, combining
stratified sampling and importance sampling.  The integration
region is divided into a number of ``boxes'', with each box getting
in fixed number of points (the goal is 2).  Each box can then have
a fractional number of bins, but if bins/box is less than two,
Vegas switches to a kind variance reduction (rather than importance
sampling). 

VEGAS:
@deftypevr {Control} double alpha
For Vegas, @code{alpha} controls the stiffness of the rebinning algorithm:
@code{alpha = 0} means never rebin.  It is typically set between 
one and two.
@end deftypevr
@deftypevr {Control} double acc
Setting @code{acc} allows vegas to terminate when the desired accuracy
has been achieved, rather than after a certain number of function calls.
Setting it to a negative value disables this feature.
@end deftypevr
@deftypevr {Control} {long int} max_it_num
The maximum number of iterations to perform.
@end deftypevr
@deftypevr {Control} int stage
Setting this determines the "stage" of the calculation.  Normally,
@code{stage = 0}.  Calling vegas with @code{stage = 1} retains
the grid (but not the answers) from the previous run, so that one
can ``tune'' the grid using a relatively small number of points and 
then do a large run with @code{stage = 1} on the optimized grid.  Setting
@code{stage = 2} keeps the grid and the answers from the previous run and
@code{stage = 3} enters at the main loop, so that nothing is changed -- this
is like deciding to change @code{max_it_num} during the run.
@end deftypevr
@deftypevr {Control} int mode
The possible choices are @code{GSL_VEGAS_MODE_IMPORTANCE}, 
        @code{GSL_VEGAS_MODE_STRATIFIED}, 
        @code{GSL_VEGAS_MODE_IMPORTANCE_ONLY}. 
This determines whether vegas will use importance sampling or stratified
sampling, or whether it can pick on its own.  In low dimensions Vegas
uses strict stratified sampling (more precisely, stratified sampling is
chosen if there are fewer than 2 bins per box).
@end deftypevr








@node Monte Carlo Examples
@section Examples

The following examples use the Monte Carlo routines to estimate the
value of a 3-dimensional integral which occurs in the theory of
random-walks.  The integral is given by,

@tex
\beforedisplay
$$
I = \int_{-\pi}^{+\pi} {dk_x \over 2\pi} 
    \int_{-\pi}^{+\pi} {dk_y \over 2\pi} 
    \int_{-\pi}^{+\pi} {dk_z \over 2\pi} 
     { 1 \over 1 - \cos(k_x)\cos(k_y)\cos(k_z)}
$$
\afterdisplay
@end tex
@ifinfo
@example
I = \int_@{-pi@}^@{+pi@} @{dk_x/(2 pi)@} 
    \int_@{-pi@}^@{+pi@} @{dk_y/(2 pi)@} 
    \int_@{-pi@}^@{+pi@} @{dk_z/(2 pi)@} 
     1 / (1 - cos(k_x)cos(k_y)cos(k_z))
@end example
@end ifinfo
@noindent
The analytic value of this integral is @math{\Gamma(1/4)^4/(4 \pi^3) =
1.393203929685676859...}.  This is the mean time spent at the origin by
a random walk on a body-centered cubic lattice in three dimensions.

@example
#include <math.h>
#include <stdio.h>

#include <gsl/gsl_math.h>
#include <gsl/gsl_monte_vegas.h>

double f0(double x[]) 
@{
  int i;
  double product = 1.0;

  for (i = 0; i < 10; i++) 
    product *= x[i];

  return product;
@}

main () 
@{
  double xl[10] = @{0, 0, 0, 0, 0, 0, 0, 0, 0, 0@};
  double xu[10] = @{1, 1, 1, 1, 1, 1, 1, 1, 1, 1@};

  double res = 0;
  double err = 0;
  double chisq = 0;
  int status = 0;
  unsigned long calls = 10000;
  unsigned long dimension = 10;

  gsl_monte_vegas_state* s = gsl_monte_vegas_alloc(10);
  gsl_monte_vegas_init(s);

  s->alpha = 1.5; /* default */
  s->verbose = 0; /* default, we prefer to remain ignorant! */
  s->stage = 0; /* default.  Start at the beginning */
  s->acc = -1.0; /* default. don't terminate when accuracy is reached */
  s->max_it_num = 5; /* default.  Do five iterations before quitting */
  
  status = gsl_monte_vegas_integrate(s, f0, xl, xu, dimension, calls,
                                    &res, &err, &chisq);
  if (status)
    printf("oops!\n");
  else
    printf("vegas(f0) = %f +- %f with \"chisq\" %f\n", res, err, chisq);

@}
@end example


@node Monte Carlo Integration References and Further Reading
@section References and Further Reading

@noindent
The @sc{miser} algorithm is described in the following article,

@itemize @asis
@item
@item
W.H. Press, G.R. Farrar, @cite{Recursive Stratified Sampling for
Multidimensional Monte Carlo Integration},
Computers in Physics, v4 (1990), pp190-195.
@end itemize
@noindent
The @sc{vegas} algorithm is described in the following papers,

@itemize @asis
@item
G.P. Lepage,
@cite{A New Algorithm for Adaptive Multidimensional Integration},
Journal of Computational Physics 27, 192-203, (1978)

@item
G.P. Lepage,
@cite{VEGAS: An Adaptive Multi-dimensional Integration Program},
Cornell preprint CLNS 80-447, March 1980
@end itemize

@noindent



