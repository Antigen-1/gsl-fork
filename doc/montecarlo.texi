@cindex Monte Carlo integration

This chapter describes routines for multidimensional Monte Carlo
integration.  These include the traditional Monte Carlo method and
adaptive algorithms such as @sc{vegas} and @sc{miser}, which use
importance sampling and stratified sampling.

Each algorithm computes an estimate of a definite integral of the form,

@tex
\beforedisplay
$$
I = \int_{xl}^{xu} \int_{yl}^{yu} ... dx\,dy ... f(x,y,...)
$$
\afterdisplay
@end tex
@ifinfo
@example
I = \int_xl^xu \int_yl^yu ... dx dy ..  f(x, y, ...)
@end example
@end ifinfo
@noindent
over a hypercubic region (xl,xu) (yl,yu) ... using a fixed number of
function calls specified by the user. Each routine also provides an
estimate of the error on the result.  These error estimates should be
taken as a guide rather than as a strict error bound, since random
sampling of the region may not uncover all the important features of the
function.

@menu
* Monte Carlo Interface::       
* PLAIN (or Simple) Monte Carlo::  
* MISER::                       
* VEGAS::                       
* Monte Carlo Examples::        
* Monte Carlo Integration References and Further Reading::  
@end menu


@node Monte Carlo Interface
@section Interface
All of the Monte Carlo integration routines use the same interface.
There is an allocator to allocate memory to hold control variables and
workspace, a routine to initialize those control variables, the
integrator itself, and a function to free the space when done.  Each
integration function requires a random number generator to be supplied,
and returns an estimate of the integral and its standard deviation.  The
accuracy of the result is determined by the number of function calls
specified by the user.  If a known level of accuracy is required this
can be achieved by calling the integrator several times and averaging
the results, or by estimating the number of calls required from the
results of an initial run with a small number of calls.

@node PLAIN (or Simple) Monte Carlo
@section PLAIN (or Simple) Monte Carlo
@cindex plain monte carlo
We begin by establishing some notation.  Let @math{I(f)} denote the
integral of the function @math{f} (for convenience, we take @math{I(1)=1})
We will write the Monte Carlo estimate of the integral as
@math{E(f; N)} for the

@tex
\beforedisplay
$$
E(f; N) = {1 \over N} \sum_i^N f(x_i).
$$
\afterdisplay
@end tex
@ifinfo
@example
E(f; N) = @{1 \over N @} \sum_i^N f(x_i).
@end example
@end ifinfo
@noindent
for @math{N} randomly distributed points @math{x_i} Similarly, we will
denote the variance of @math{f} by

@tex
\beforedisplay
$$
\sigma^2(f) = I(f^2) - I(f)^2
$$
\afterdisplay
@end tex
@ifinfo
@example
\sigma^2(f) = I(f^2) - I(f)^2
@end example
@end ifinfo
@noindent
and the variance of the estimate by 

@tex
\beforedisplay
$$
Var(f; N) = E(f^2; N) - (E(f; N))^2.
$$
\afterdisplay
@end tex
@ifinfo
@example
Var(f; N) = E(f^2; N) - (E(f; N))^2.
@end example
@end ifinfo

The fundamental point of Monte Carlo integration is that for
large @math{N}, 

@tex
\beforedisplay
$$
E(f; N) - I(f) \approx {\sigma^2(f)\over N}
$$
\afterdisplay
@end tex
@ifinfo
@example
E(f; N) - I(f) \approx @{\sigma^2(f)\over N @}
@end example
@end ifinfo
@noindent
and furthermore, by the same argument, 

@tex
\beforedisplay
$$
\sigma^2(f) \approx Var(f; N)
$$
\afterdisplay
@end tex
@ifinfo
@example
\sigma^2(f) \approx Var(f; N)
@end example
@end ifinfo

That's all there is to simple Monte Carlo.

@deftypefun {gsl_monte_plain_state *} gsl_monte_plain_alloc (size_t @var{dim})
This function allocates and initializes a workspace for Monte Carlo
integration in @var{dim} dimensions.  The workspace is used to maintain
the state of the integration.
@end deftypefun

@deftypefun int gsl_monte_plain_init (gsl_monte_plain_state* @var{s})
This function initializes a previously allocated integration state.
This allows an existing workspace to be reused for different
integrations.
@end deftypefun

@deftypefun int gsl_monte_plain_integrate (gsl_monte_function * @var{f}, double * @var{xl}, double * @var{xu}, size_t @var{dim}, size_t @var{calls}, gsl_rng * @var{r}, gsl_monte_plain_state * @var{s}, double * @var{result}, double * @var{abserr})
This routines uses the plain Monte Carlo algorithm to integrate the
function @var{f} over the @var{dim}-dimensional hypercubic region
defined by the lower and upper limits in the arrays @var{xl} and
@var{xu}, each of size @var{dim}.  The integration uses a fixed number
of function calls @var{calls}, and obtains random sampling points using
the random number generator @var{r}. A previously allocated workspace
@var{s} must be supplied.  The result of the integration is returned in
@var{result}, with an estimated absolute error @var{abserr}.
@end deftypefun

@deftypefun void gsl_monte_plain_free (gsl_monte_plain_state* @var{s}), 
This function frees the memory associated with the integrator state
@var{s}.
@end deftypefun

@node MISER
@section MISER
@cindex MISER monte carlo integration
@cindex recursive stratified sampling, MISER

The starting point of all stratified sampling techniques is
the observation that for two disjoint regions @math{a} and @math{b} with
Monte Carlo estimates of the integral @math{E_a(f)} and @math{E_b(f)} of the 
integral of f in those regions
and variances @math{\sigma_a^2(f)} and @math{\sigma_b^2(f)}, 
the variance @math{Var(f)}
of the combined estimate 
@c{$E(f) = {1\over 2} E_a(f) + {1\over 2} E_b(f)$}
@math{E(f) = @{1\over 2@} E_a(f) + @{1\over 2@} E_b(f)}
is given by

@tex
\beforedisplay
$$
Var(f) = {\sigma_a^2(f) \over 4 N_a} + {\sigma_b^2(f) \over 4 N_b}
$$
\afterdisplay
@end tex
@ifinfo
@example
Var(f) = @{\sigma_a^2(f) \over 4 N_a@} + @{\sigma_b^2(f) \over 4 N_b@}
@end example
@end ifinfo

This variance is minimized (subject to the constrain that @math{N_a + N_b} 
is fixed) by choosing (assuming one can) @math{N_a} s.t.

@tex
\beforedisplay
$$
{N_a \over N_a+N_b} = {\sigma_a \over \sigma_a + \sigma_b}
$$
\afterdisplay
@end tex
@ifinfo
@example
@{N_a \over N_a+N_b @} = @{ \sigma_a \over \sigma_a + \sigma_b @}
@end example
@end ifinfo

For such a choice, the variance is given by

@tex
\beforedisplay
$$
Var(f) = {(\sigma_a + \sigma_b)^2 \over 4 N}
$$
\afterdisplay
@end tex
@ifinfo
@example
Var(f) =  @{ (\sigma_a + \sigma_b)^2 \over 4 N @}
@end example
@end ifinfo

In words, the variance of the estimate (and hence the error) is
minimized by concentrating points in regions where the variance of the
function @math{f} is large.

The most straightforward stratified sampling routine divides a
hyper-cubic integration region into sub-cubes along the coordinate axes.
This is simple to do, but gets out of hand for large numbers of
dimensions @math{d} because the number of sub-cubes grows like 
@math{K^d} (where @math{K} is the number of sub-divisions along the
axis).  

What MISER does (and other recursive stratified samplers) is to bisect
the hypercube along one coordinate axis.  The direction is chosen by
examining all possible bisections (@math{d} of them) and picking the one
that gives the best combined variance.  The same procedure is then done
for each of the two half-spaces (choosing @math{N_a} and @math{N_b} as
described above), and so on.  Since by assumption one does not know the
variance @math{Var_L(f)} in the various sub-regions, it is estimated
using some fraction of the total number of points alloted to the
estimate.

@deftypefun {gsl_monte_miser_state *} gsl_monte_miser_alloc (size_t @var{dim})
This function allocates and initializes a workspace for Monte Carlo
integration in @var{dim} dimensions.  The workspace is used to maintain
the state of the integration.
@end deftypefun

@deftypefun int gsl_monte_miser_init (gsl_monte_miser_state* @var{s})
This function initializes a previously allocated integration state.
This allows an existing workspace to be reused for different
integrations.
@end deftypefun

@deftypefun int gsl_monte_miser_integrate (gsl_monte_function * @var{f}, double * @var{xl}, double * @var{xu}, size_t @var{dim}, size_t @var{calls}, gsl_rng * @var{r}, gsl_monte_miser_state * @var{s}, double * @var{result}, double * @var{abserr})
This routines uses the @sc{miser} Monte Carlo algorithm to integrate the
function @var{f} over the @var{dim}-dimensional hypercubic region
defined by the lower and upper limits in the arrays @var{xl} and
@var{xu}, each of size @var{dim}.  The integration uses a fixed number
of function calls @var{calls}, and obtains random sampling points using
the random number generator @var{r}. A previously allocated workspace
@var{s} must be supplied.  The result of the integration is returned in
@var{result}, with an estimated absolute error @var{abserr}.
@end deftypefun

@deftypefun void gsl_monte_miser_free (gsl_monte_miser_state* @var{s}), 
This function frees the memory associated with the integrator state
@var{s}.
@end deftypefun


MISER:
@deftypevr {Control} double alpha
alpha controls how the variances for the two sub-regions are combined.
The Numerical Recipes gang argue that for recursive sampling there
is no reason to expect that the variance should scale like @math{1/N} and
so they allow the scaling to depend on @math{\alpha}

@tex
\beforedisplay
$$
Var(f) = {\sigma_a \over N_a^\alpha} + {\sigma_b \over N_b^\alpha}
$$
\afterdisplay
@end tex
@ifinfo
@example
Var(f) = @{\sigma_a \over N_a^\alpha@} + @{\sigma_b \over N_b^\alpha@}
@end example
@end ifinfo

@end deftypevr

@deftypevr {Control} double dither
Rather than exactly bisection the integration region, dither allows the
user to introduce a bit of fuzz.  This helps in the case when the
function to be integrated has some symmetry, say if it is peaked in the
center of the hypercube.  If needed, dither should be around 0.1.
@end deftypevr 


@node VEGAS
@section VEGAS
@cindex VEGAS monte carlo integration
@cindex importance sampling, VEGAS

Vegas takes another approach to obtaining good results, namely it
tries to sample points from the probability distribution described
by the function @math{f}.  More precisely, suppose we estimate
the integral of @math{f} with points distributed according to
a probability distribution described by the function @math{g}.  If
we call the new estimate @math{E_g(f; N)} we have

@tex
\beforedisplay
$$
E_g(f; N) = E(f/g; N)
$$
\afterdisplay
@end tex
@ifinfo
@example
E_g(f; N) = E(f/g; N)
@end example
@end ifinfo

and similarly

@tex
\beforedisplay
$$
Var_g(f; N) = Var(f/g; N)
$$
\afterdisplay
@end tex
@ifinfo
@example
Var_g(f; N) = Var(f/g; N)
@end example
@end ifinfo

It is clear from this that if @math{g = |f|/I(|f|)} then the variance 
@math{V_g(f; N)} vanishes.
This it was what Vegas attempts to do.  It makes several passes,
histograming @math{f}, and each time using the histogram to define the
sampling distribution for the next pass.  This basic idea again has the
problem that the number of histogram bins grows like 
@math{K^d}.  The
compromise that Vegas (ie, Peter Lepage) adopts is to assume that
@math{g} factors: 
@c{$g(x_1, x_2, \ldots) = g_1(x_1) g_2(x_2) \ldots$}
@math{g(x_1, x_2, ...) = g_1(x_1) g_2(x_2) ...}
so that the number of bins required is only 
@c{$d \cdot K$}
@math{d.K}.  In this
case, it is not hard to show that the optimal distribution is 

@tex
\beforedisplay
$$ 
g_1(x_1) = [\int dx_1 \ldots dx_n {f^2(x_1,\dots,x_n)\over g_2(x_2)\ldots g_n(x_n) } ]^{(1/2)}
$$
\afterdisplay
@end tex
@ifinfo
@example
g_1(x_1) = [\int dx_1 ... dx_n @{f^2(x_1,...,x_n)\over g_2(x_2)... g_n(x_n) @} ]^@{(1/2)@}
@end example
@end ifinfo

Lepage's Vegas is actually a bit fancier than this, combining
stratified sampling and importance sampling.  The integration
region is divided into a number of ``boxes'', with each box getting
in fixed number of points (the goal is 2).  Each box can then have
a fractional number of bins, but if bins/box is less than two,
Vegas switches to a kind variance reduction (rather than importance
sampling). 


@deftypefun {gsl_monte_vegas_state *} gsl_monte_vegas_alloc (size_t @var{dim})
This function allocates and initializes a workspace for Monte Carlo
integration in @var{dim} dimensions.  The workspace is used to maintain
the state of the integration.
@end deftypefun

@deftypefun int gsl_monte_vegas_init (gsl_monte_vegas_state* @var{s})
This function initializes a previously allocated integration state.
This allows an existing workspace to be reused for different
integrations.
@end deftypefun

@deftypefun int gsl_monte_vegas_integrate (gsl_monte_function * @var{f}, double * @var{xl}, double * @var{xu}, size_t @var{dim}, size_t @var{calls}, gsl_rng * @var{r}, gsl_monte_vegas_state * @var{s}, double * @var{result}, double * @var{abserr})
This routines uses the @sc{vegas} Monte Carlo algorithm to integrate the
function @var{f} over the @var{dim}-dimensional hypercubic region
defined by the lower and upper limits in the arrays @var{xl} and
@var{xu}, each of size @var{dim}.  The integration uses a fixed number
of function calls @var{calls}, and obtains random sampling points using
the random number generator @var{r}. A previously allocated workspace
@var{s} must be supplied.  The result of the integration is returned in
@var{result}, with an estimated absolute error @var{abserr}.
@end deftypefun

@deftypefun void gsl_monte_vegas_free (gsl_monte_vegas_state* @var{s}), 
This function frees the memory associated with the integrator state
@var{s}.
@end deftypefun


VEGAS:
@deftypevr {Control} double alpha
For Vegas, @code{alpha} controls the stiffness of the rebinning algorithm:
@code{alpha = 0} means never rebin.  It is typically set between 
one and two.
@end deftypevr
@deftypevr {Control} double acc
Setting @code{acc} allows vegas to terminate when the desired accuracy
has been achieved, rather than after a certain number of function calls.
Setting it to a negative value disables this feature.
@end deftypevr
@deftypevr {Control} {long int} max_it_num
The maximum number of iterations to perform.
@end deftypevr
@deftypevr {Control} int stage
Setting this determines the "stage" of the calculation.  Normally,
@code{stage = 0}.  Calling vegas with @code{stage = 1} retains
the grid (but not the answers) from the previous run, so that one
can ``tune'' the grid using a relatively small number of points and 
then do a large run with @code{stage = 1} on the optimized grid.  Setting
@code{stage = 2} keeps the grid and the answers from the previous run and
@code{stage = 3} enters at the main loop, so that nothing is changed -- this
is like deciding to change @code{max_it_num} during the run.
@end deftypevr
@deftypevr {Control} int mode
The possible choices are @code{GSL_VEGAS_MODE_IMPORTANCE}, 
        @code{GSL_VEGAS_MODE_STRATIFIED}, 
        @code{GSL_VEGAS_MODE_IMPORTANCE_ONLY}. 
This determines whether vegas will use importance sampling or stratified
sampling, or whether it can pick on its own.  In low dimensions Vegas
uses strict stratified sampling (more precisely, stratified sampling is
chosen if there are fewer than 2 bins per box).
@end deftypevr

@node Monte Carlo Examples
@section Examples

The following example program uses the Monte Carlo routines to estimate
the value of a 3-dimensional integral which occurs in the theory of
random-walks.  The integral is given by,

@tex
\beforedisplay
$$
I = \int_{-\pi}^{+\pi} {dk_x \over 2\pi} 
    \int_{-\pi}^{+\pi} {dk_y \over 2\pi} 
    \int_{-\pi}^{+\pi} {dk_z \over 2\pi} 
     { 1 \over 1 - \cos(k_x)\cos(k_y)\cos(k_z)}
$$
\afterdisplay
@end tex
@ifinfo
@example
I = \int_@{-pi@}^@{+pi@} @{dk_x/(2 pi)@} 
    \int_@{-pi@}^@{+pi@} @{dk_y/(2 pi)@} 
    \int_@{-pi@}^@{+pi@} @{dk_z/(2 pi)@} 
     1 / (1 - cos(k_x)cos(k_y)cos(k_z))
@end example
@end ifinfo
@noindent
The analytic value of this integral is @math{\Gamma(1/4)^4/(4 \pi^3) =
1.393203929685676859...}.  This is the mean time spent at the origin by
a random walk on a body-centered cubic lattice in three dimensions.

@example
#include <stdlib.h>
#include <gsl/gsl_math.h>
#include <gsl/gsl_monte.h>
#include <gsl/gsl_monte_plain.h>
#include <gsl/gsl_monte_miser.h>
#include <gsl/gsl_monte_vegas.h>

/* Computation of the integral,

      I = int (dx dy dz)/(2pi)^3  1/(1-cos(x)cos(y)cos(z))

   over (-pi,-pi,-pi) to (+pi, +pi, +pi).  The exact answer is
   Gamma(1/4)^4/(4 pi^3).  This example is taken from C.Itzykson,
   J.M.Drouffe, "Statistical Field Theory - Volume 1", Section 1.1,
   p21, which cites the original paper M.L.Glasser, I.J.Zucker,
   Proc.Natl.Acad.Sci.USA 74 1800 (1977) */

double exact = 1.3932039296856768591842462603255;

double
g (double *k, size_t dim, void *params)
@{
  double A = 1.0 / (M_PI * M_PI * M_PI);
  return A / (1.0 - cos (k[0]) * cos (k[1]) * cos (k[2]));
@}

void
display_results (char *title, double result, double error)
@{
  printf ("%s ==================\n", title);
  printf ("result = % .6f\n", result);
  printf ("sigma  = % .6f\n", error);
  printf ("exact  = % .6f\n", exact);
  printf ("error  = % .6f = %.1g sigma\n", result - exact,
	  fabs (result - exact) / error);
@}

int
main ()
@{
  double res, err;

  double xl[3] = @{ 0, 0, 0 @};
  double xu[3] = @{ M_PI, M_PI, M_PI @};

  gsl_rng *r;

  gsl_monte_function G = @{ &g, 3, 0 @};

  size_t calls = 100000;

  gsl_rng_env_setup ();

  r = gsl_rng_alloc (gsl_rng_default);

  @{
    gsl_monte_plain_state *s = gsl_monte_plain_alloc (3);
    gsl_monte_plain_integrate (&G, xl, xu, 3, 5 * calls, r, s, &res, &err);
    gsl_monte_plain_free (s);

    display_results ("plain", res, err);
  @}

  @{
    gsl_monte_miser_state *s = gsl_monte_miser_alloc (3);
    gsl_monte_miser_integrate (&G, xl, xu, 3, 5 * calls, r, s, &res, &err);
    gsl_monte_miser_free (s);

    display_results ("miser", res, err);
  @}

  @{
    gsl_monte_vegas_state *s = gsl_monte_vegas_alloc (3);

    gsl_monte_vegas_integrate (&G, xl, xu, 3, 10000, r, s, &res, &err);
    display_results ("vegas warm-up", res, err);

    printf ("converging...\n");

    do
      @{
	gsl_monte_vegas_integrate (&G, xl, xu, 3, calls, r, s, &res, &err);
	printf ("result = % .6f sigma = % .6f chisq/dof = %.1f\n",
		res, err, s->chisq);
      @}
    while (fabs (s->chisq - 1.0) > 0.5);

    display_results ("vegas final", res, err);

    gsl_monte_vegas_free (s);
  @}
@}
@end example

@example
plain ==================
result =  1.385867
sigma  =  0.007938
exact  =  1.393204
error  = -0.007337 = 0.9 sigma
miser ==================
result =  1.390656
sigma  =  0.003743
exact  =  1.393204
error  = -0.002548 = 0.7 sigma
vegas warm-up ==================
result =  1.386925
sigma  =  0.002651
exact  =  1.393204
error  = -0.006278 = 2 sigma
converging...
result =  1.392957 sigma =  0.000452 chisq/dof = 1.1
vegas final ==================
result =  1.392957
sigma  =  0.000452
exact  =  1.393204
error  = -0.000247 = 0.5 sigma
@end example

@node Monte Carlo Integration References and Further Reading
@section References and Further Reading

@noindent
The @sc{miser} algorithm is described in the following article,

@itemize @asis
@item
@item
W.H. Press, G.R. Farrar, @cite{Recursive Stratified Sampling for
Multidimensional Monte Carlo Integration},
Computers in Physics, v4 (1990), pp190-195.
@end itemize
@noindent
The @sc{vegas} algorithm is described in the following papers,

@itemize @asis
@item
G.P. Lepage,
@cite{A New Algorithm for Adaptive Multidimensional Integration},
Journal of Computational Physics 27, 192-203, (1978)

@item
G.P. Lepage,
@cite{VEGAS: An Adaptive Multi-dimensional Integration Program},
Cornell preprint CLNS 80-447, March 1980
@end itemize

@noindent



