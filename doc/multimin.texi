@cindex minimization, multidimensional

This chapter describes functions for finding minima of arbitrary
multidimensional functions.  The library provides low level components
for a variety of iterative minimizers and convergence tests.  These
can be combined by the user to achieve the desired solution, while
providing full access to the intermediate steps of the algorithms.
Each class of methods uses the same framework, so that you can switch
between minimizers at runtime without needing to recompile your
program.  Each instance of a minimizer keeps track of its own state,
allowing the minimizers to be used in multi-threaded programs.

The header file @file{gsl_multimin.h} contains prototypes for the
minimization functions and related declarations.  To use the
minimization algorithms to find the maximum of a function simply
invert its sign.

@menu
* Multimin Overview::       
* Multimin Caveats::        
* Initializing the Multidimensional Minimizer::  
* Providing a function to minimize::  
* Multimin Iteration::      
* Multimin Stopping Criteria::  
* Multimin Algorithms::     
* Multimin Examples::       
* Multimin References and Further Reading::  
@end menu

@node Multimin Overview
@section Overview

The multidimensional minimization routines build on the one-dimensional
minimization algorithms to create a flexible framework for finding local
minima.  The library minimizes functions in @math{N} dimensions using a
variety of gradient search algorithms.  In these algorithms, the
gradient of the function to be minimized is first calculated at the
current best estimate of the minimum of the function. The gradient is
used to determine a downhill search direction for the next iteration of
the minimizer.  A line search is made along this direction to hunt for
an improved minimum and the best estimate is updated to that point. This
process is repeated until the location of the minimum is determined to
sufficient accuracy.  There are other algorithms that can be used for
finding local minima of @math{N}-dimensional functions and these may be
added in future.

A variety of one-dimensional algorithms are available within the
minimization framework.  The user provides a high-level driver for the
algorithms, and the library provides the individual functions necessary
for each of the steps.  There are three main phases of the iteration.
The steps are,

@itemize @bullet
@item
initialize minimizer state, @var{s}, for algorithm @var{T}

@item
update @var{s} using the iteration @var{T}

@item
test @var{s} for convergence, and repeat iteration if necessary
@end itemize

@noindent
The state for the minimizers is held in a @code{gsl_min_fminimizer}
struct.  The updating procedure uses only function evaluations (not
derivatives).

@node Multimin Caveats
@section Caveats
@cindex Multimin, caveats

Note that minimization functions can only search for one local minimum
at a time.  When there are several local minima in the search area, the
first minimum to be found will be returned; however it is difficult to
predict which of the minima this will be.  In most cases, no error will
be reported if you try to find a local minimum in an area where there is
more than one.

It is also important to note that the minimization algorithms find local
minima; there is no way to determine whether a minimum is a global
minimum of the function in question.

@node Initializing the Multidimensional Minimizer
@section Initializing the Multidimensional Minimizer

@deftypefun {gsl_multimin_fdfminimizer *} gsl_multimin_fdfminimizer_alloc (const gsl_multimin_fdf_function * @var{fdf}, const gsl_vector * @var{x}, gsl_min_bracketing_function @var{bracket}, const gsl_multimin_fdfminimizer_type * @var{T})

This function returns a pointer to a a newly allocated instance of a
minimizer of type @var{T} for the function @var{fdf}.  For example,
the following code creates an instance of a steepest descent
minimizer using a Brent minimizer for the line search.

@example
s = gsl_multimin_fdfminimizer_alloc(
        gsl_multimin_fdfminimizer_steepest_descent, 
        &fdf, 
        x,                         
        gsl_min_find_bracket,         
        gsl_min_fminimizer_brent);
@end example

If there is insufficient memory to create the minimizer then the function
returns a null pointer and the error handler is invoked with an error
code of @code{GSL_ENOMEM}.
@end deftypefun

@deftypefun void gsl_multimin_fdfminimizer_free (gsl_multimin_fdfminimizer * @var{s})

This function frees all the memory associated with the minimizer
@var{s}.
@end deftypefun

@deftypefun {const char *} gsl_multimin_fdfminimizer_name (const gsl_multimin_fdfminimizer * @var{s})

This function returns a pointer to the name of the minimizer.  For example,

@example
printf("s is a '%s' minimizer\n", 
       gsl_multimin_fdfminimizer_name (s));
@end example

@noindent
would print something like @code{s is a 'conjugate_pr' minimizer}.
@end deftypefun

@node Providing a function to minimize
@section Providing a function to minimize

You must provide a parametric function of @math{N} variables for the
minimizers to operate on.  You also need to provide a routine which
calculates the gradient of the function, a third routine which
calculates both the function value and the gradient, the dimensionality
of the function domain and an array of parameters.  The
@code{gsl_multimin_function_fdf} type encapsulates these objects in a
single struct.  The following example defines a function suitable for
the case of a simple paraboloid,

@example
static double
my_f (const gsl_vector *v, void *params)
@{
  double x, y;
  double *dp = (double *)params;
  
  x = gsl_vector_get(v, 0);
  y = gsl_vector_get(v, 1);
 
  return (x - *dp) * (x - *dp) +
         (y - *(dp + 1)) * (y - *(dp + 1)); 
@}

/* The gradient of f, df = (df/dx, df/dy). */
static void 
my_df (const gsl_vector *v, void *params, 
       gsl_vector *df)
@{
  double x, y;
  double *dp = (double *)params;
  
  x = gsl_vector_get(v, 0);
  y = gsl_vector_get(v, 1);
 
  gsl_vector_set(df, 0, 20.0 * (x - *dp));
  gsl_vector_set(df, 1, 40.0 * (y - *(dp+1)));
@}

/* Now both f and df together. */
static void 
my_fdf (const gsl_vector *x, void *params, 
        double *f, gsl_vector *df) 
@{
  *f = my_f(x, params); 
  my_df(x, params, df);
@}

int
main(void)
@{
  gsl_multimin_function_fdf my_func;

  my_func.f = &my_f;
  my_func.df = &my_df;
  my_func.fdf = &my_fdf;
  my_func.n = 2;
  my_func.params = NULL;
@}
@end example

@node Multimin Iteration
@section Iteration

Multidimensional minimization proceeds as a pair of nested iterations.
An outer loop iterates over successive estimates of the location of the
minimum of the objective function; at each new point the gradient of the
objective function is calculated.  The gradient is used to determine a
new direction along which to move.  The different algorithms in the
multimin package use the gradient in different ways in calculating this
new direction.  Whatever the method employed, the inner loop hunts for a
minimum along that direction.

@node Multimin Stopping Criteria
@section Stopping Criteria

The gradient of a multidimensional function goes to zero at a minimum,
in the same way that the first derivative of a one dimensional function
goes to zero at a minimum.  In exact arithmetic, finding the point at
which the norm of the gradient is zero would be a sufficient condition
for locating an extremum (or an inflection point).  In practical
numerical calculations, additional criteria are often necessary.  If the
function is particularly shallow in the neighborhood of the minimum, or
if the round-off error in the function evaluation is large, it is not
always possible to rely solely on the norm of the gradient as a stopping
criterion.  An additional requirement which can make the stopping
criteria more robust is to demand that the estimate of the location of
the minimum on successive iterations differ by more than some minimum
distance.

@node Multimin Algorithms
@section Algorithms

There are several minimization methods available in the multimin
package.  Each of the algorithms relies on knowing the value of the
function and its gradient at each evaluation point. The choice of which
algorithm is the best to use depends on the details of the problem.  It
is sometimes the case that the best approach is to start out with one
algorithm and switch to another when the estimate of the minimum
approaches the true minimum.

@deffn {Minimization Method} gsl_multimin_fdfminimizer_steepest_descent
The steepest descent method is the most straightforward of the
available algorithms.  When a point is examined during the hunt for
the minimum, the gradient is calculated at that point and the next
line search proceeds along the direction of the gradient.  After the
line search has found a minimum, the process is repeated.  Since the
new point will be at a minimum of the function as it is varied along
the direction of the old gradient, the new direction of steepest
descent will be orthogonal to the old direction.  While this method is
robust and does not depend in detail on the shape of the function in
the neighborhood of the minimum, it can sometimes be slow to converge.
@end deffn

The remaining methods use the conjugate gradient algorithm.  The
conjugate gradient algorithm uses information from previous iterations
to build up a more complete picture of the curvature of the function in
the neighborhood of the minimum.  During each iteration, the gradient at
the current evaluation point and a scaled gradient from the previous
iteration are combined to produce the new search direction.

@deffn {Minimizer} gsl_multimin_fdfminimizer_conjugate_pr
The Polak-Ribiere conjugate gradient algorithm.
@end deffn

@deffn {Minimizer} gsl_multimin_fdfminimizer_conjugate_fr
The Fletcher-Reeves conjugate gradient algorithm.
@end deffn

The Polak-Ribiere method and the Fletcher-Reeves method differ in the
way in which they calculate the scaling coefficient.  Both work well
when the evaluation point is close enough to the minimum of the
objective function that it is well approximated by a quadratic
hypersurface.  Some references suggest starting out with the steepest
descent algorithm and switching to one of the conjugate gradient
methods as the estimate of the minimum improves.

@deffn {Minimizer} gsl_multimin_fdfminimizer_vector_bfgs
The Broyden-Fletcher-Goldfarb-Shanno conjugate gradient method.
@end deffn

@node Multimin Examples
@section Examples

This example attempts to find the minimum of a paraboloid.  The location
of the minimum is offset from the origin in @code{x} and @code{y}, and
the function value at the minimum is non-zero.  The example shows the
typical nested iteration of a multidimensional minimization routine.
The first part of the program defines the function to be minimized,

@smallexample
/* The function we want to minimize.  It has a minimum at
   f(dp[0],dp[1]) = 30.0 */
static double
my_f (const gsl_vector * v, void *params)
@{
  double x, y;
  double * dp = (double *) params;

  x = gsl_vector_get (v, 0);
  y = gsl_vector_get (v, 1);

  return 10.0 * (x - dp[0]) * (x - dp[0])
    + 20.0 * (y - dp[1]) * (y - dp[1]) + 30.0;
@}

/* The gradient of f, df = (df/dx, df/dy). */
static void
my_df (const gsl_vector * v, void *params, gsl_vector * df)
@{
  double x, y;
  double *dp = (double *) params;

  x = gsl_vector_get (v, 0);
  y = gsl_vector_get (v, 1);

  gsl_vector_set (df, 0, 20.0 * (x - dp[0]));
  gsl_vector_set (df, 1, 40.0 * (y - dp[1]));
@}

/* Calculate both f and df. */
static void
my_fdf (const gsl_vector * x, void *params, double *f, gsl_vector * df)
@{
  *f = my_f (x, params);
  my_df (x, params, df);
@}
@end smallexample
@noindent
The main program is given below,
@smallexample
#include <stdio.h>
#include <gsl/gsl_min.h>
#include <gsl/gsl_multimin.h>

const double EPSABS = 1e-4;
const double EPSREL = 1e-4;
const unsigned int MAX_ITER = 10000;

int
main (void)
@{
  size_t i = 0, j = 0;
  int status;
  double minimum, a, b;
  const gsl_multimin_fdfminimizer_type *T;
  gsl_multimin_fdfminimizer *s;

  /* Set position of the minimum, v = (1,2). */
  double par[2] = @{ 1.0, 2.0 @};

  gsl_vector *x;
  gsl_multimin_function_fdf my_func;

  my_func.f = &my_f;
  my_func.df = &my_df;
  my_func.fdf = &my_fdf;
  my_func.n = 2;
  my_func.params = &par;

  /* Initial guess for the location of the minimum, x = (10,10) */

  x = gsl_vector_alloc (2);
  gsl_vector_set (x, 0, 10.0);
  gsl_vector_set (x, 1, 10.0);

  /* Allocate a multidimensional minimizer. */
  T = gsl_multimin_fdfminimizer_conjugate_fr;
  s = gsl_multimin_fdfminimizer_alloc (T, &my_func, x,
                                       gsl_min_find_bracket,
                                       gsl_min_fminimizer_brent);
  do
    @{
      i++;

      gsl_multimin_fdfminimizer_next_direction (s);

      status = gsl_multimin_fdfminimizer_bracket (s, 10.0, 50);

      if (status == GSL_FAILURE)
	break;

      j = 0;

      do
	@{
	  j++;
	  status = gsl_multimin_fdfminimizer_iterate (s);

	  minimum = gsl_min_fminimizer_minimum (s->line_search);
	  a = gsl_min_fminimizer_x_lower (s->line_search);
	  b = gsl_min_fminimizer_x_upper (s->line_search);

	  status = gsl_min_test_interval (a, b, EPSABS, EPSREL);
	@}
      while (status == GSL_CONTINUE && j < MAX_ITER);

      gsl_multimin_fdfminimizer_best_step (s);

      status = gsl_multimin_test_gradient_sqr_norm (s->history, EPSABS);

      if (status == GSL_SUCCESS)
        printf ("Minimum found at:\n");

      printf (" f(%8.5f,%8.5f) = %8.5f\n",
	      gsl_vector_get (s->history->x, 0),
	      gsl_vector_get (s->history->x, 1), s->history->f);
    @}
  while (i < MAX_ITER && status == GSL_CONTINUE);

  gsl_multimin_fdfminimizer_free (s);
  gsl_vector_free (x);

  return 0;
@}
@end smallexample  

@node Multimin References and Further Reading
@section References and Further Reading

A brief description of multidimensional minimization algorithms and
further references can be found in the following book,

@itemize @asis
@item C.W. Ueberhuber,
@cite{Numerical Computation (Volume 2)}, Chapter 14, Section 4.4
"Minimization Methods", p. 325---335, Springer (1997), ISBN
3-540-62057-5.
@end itemize
@noindent
